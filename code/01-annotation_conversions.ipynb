{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation conversions\n",
    "\n",
    "Notebook for performing a series of manipulations to the genome annotation files. Poorly documented.\n",
    "\n",
    "2020-09-14\n",
    "\n",
    "## Initial boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard variables loaded, you are good to go!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from os.path import join, dirname, basename, exists, isdir\n",
    "\n",
    "### Load environmental variables from the project root directory ###\n",
    "# find .env automagically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# now you can get the variables using their names\n",
    "\n",
    "# Check whether a network drive has been specified\n",
    "DATABASE = os.environ.get(\"NETWORK_URL\")\n",
    "if DATABASE == 'None':\n",
    "    pass\n",
    "else:\n",
    "    pass\n",
    "    #mount network drive here\n",
    "\n",
    "# set up directory paths\n",
    "CURRENT_DIR = os.getcwd()\n",
    "PROJ = dirname(dotenv_path) # project root directory\n",
    "\n",
    "DATA = join(PROJ, 'data') #data directory\n",
    "RAW_EXTERNAL = join(DATA, 'raw_external') # external data raw directory\n",
    "RAW_INTERNAL = join(DATA, 'raw_internal') # internal data raw directory\n",
    "INTERMEDIATE = join(DATA, 'intermediate') # intermediate data directory\n",
    "FINAL = join(DATA, 'final') # final data directory\n",
    "\n",
    "RESULTS = join(PROJ, 'results') # output directory\n",
    "FIGURES = join(RESULTS, 'figures') # figure output directory\n",
    "PICTURES = join(RESULTS, 'pictures') # picture output directory\n",
    "\n",
    "\n",
    "# make folders specific for certain data\n",
    "folder_name = ''\n",
    "if folder_name != '':\n",
    "    #make folders if they don't exist\n",
    "    if not exists(join(RAW_EXTERNAL, folder_name)):\n",
    "        os.makedirs(join(RAW_EXTERNAL, folder_name))\n",
    "\n",
    "    if not exists(join(INTERMEDIATE, folder_name)):\n",
    "        os.makedirs(join(INTERMEDIATE, folder_name))\n",
    "\n",
    "    if not exists(join(FINAL, folder_name)):\n",
    "        os.makedirs(join(FINAL, folder_name))\n",
    "\n",
    "print('Standard variables loaded, you are good to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Annotation file\n",
    "\n",
    "Some lines did not have gene id, which made DE tools crash. Therefore we add them here as the transcript id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_genome_name = join(RAW_EXTERNAL,'genome_annotation.gtf')\n",
    "fixed_genome_name = join(RAW_EXTERNAL,'genome_annotation_fixed.gtf')\n",
    "with open(old_genome_name) as old_genome:\n",
    "    with open(fixed_genome_name, 'w', newline='') as fixed_genome:\n",
    "        lines = []\n",
    "        for line in old_genome:\n",
    "            if \"gene_id\" not in line:\n",
    "                line = line.replace(\"transcript_id\", \"gene_id\")\n",
    "            line = line.replace(\"; \",\";\")\n",
    "            lines.append(line)\n",
    "        fixed_genome.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gene length file\n",
    "\n",
    "We calculated the gene length by substracting the end from start of each exon, and then adding up all exons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "genome_file_name = join(RAW_EXTERNAL,'genome_annotation_fixed.gtf')\n",
    "length_file_name = join(RAW_EXTERNAL,'gene_length.tabular')\n",
    "\n",
    "# create list of gene names:\n",
    "with open(genome_file_name) as genome_file:\n",
    "    gene_names = []\n",
    "    for line in csv.reader(genome_file, delimiter='\\t'):\n",
    "        if line[2] == \"exon\" and \"gene_name\" in line[8]:\n",
    "            gene_name = line[8].split(\";\")[2]\n",
    "            gene_name = gene_name.replace('gene_name ','')\n",
    "            gene_name = gene_name.replace('\"','')\n",
    "            if gene_name not in gene_names:\n",
    "                gene_names.append(gene_name)\n",
    "\n",
    "# compute length for each gene:\n",
    "with open(length_file_name, 'w', newline='') as length_file:\n",
    "    length_writer = csv.writer(length_file, delimiter='\\t')\n",
    "    for gene_name in gene_names:\n",
    "        gene_length = 0\n",
    "        with open(genome_file_name) as genome_file:\n",
    "            for line in csv.reader(genome_file, delimiter='\\t'):\n",
    "                if line[2] == \"exon\" and f'gene_name \"{gene_name}\"' in line[8]:\n",
    "                    # add length of exon\n",
    "                    gene_length += int(line[4])-int(line[3])+1\n",
    "            # write new gene:\n",
    "            length_writer.writerow([gene_name,gene_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ontology file\n",
    "\n",
    "Created file with associations 1-1 of each gene with ontologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_ontology_name = join(RAW_EXTERNAL,'ontologies.tabular')\n",
    "new_ontology_name = join(RAW_EXTERNAL,'ontologies_fixed.tabular')\n",
    "\n",
    "with open(old_ontology_name) as old_ontology:\n",
    "    with open(new_ontology_name, 'w', newline='') as new_ontology:\n",
    "        old_reader = csv.reader(old_ontology, delimiter='\\t')\n",
    "        new_writer = csv.writer(new_ontology, delimiter='\\t')\n",
    "        for line in old_reader:\n",
    "            if \"|\" in line[2]:\n",
    "                GO_list = line[2].split(\"|\")\n",
    "                for GO in GO_list:\n",
    "                    new_writer.writerow([line[1], GO])\n",
    "            else:\n",
    "                new_writer.writerow([line[1], line[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do it again but for phophoproteomics data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of phosphoproteomic ids:\n",
    "import pandas as pd\n",
    "phospho_file = join(INTERMEDIATE,'phosphoCounts_normalized.csv')\n",
    "phospho_ids = pd.read_csv(phospho_file, index_col=0, sep=\",\").index.tolist()\n",
    "\n",
    "# Load protein ids as dictionary:\n",
    "prot_ids = {}\n",
    "with open(join(RAW_EXTERNAL,'uniprot_genes.tab')) as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            (prot, gene) = line.split()\n",
    "            prot_ids[gene] = prot\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Create phophoproteomics ontology file:        \n",
    "old_ontology_name = join(RAW_EXTERNAL,'ontologies.tabular')\n",
    "new_ontology_name = join(RAW_EXTERNAL,'ontologies_phospho.tabular')\n",
    "\n",
    "with open(old_ontology_name) as old_ontology:\n",
    "    with open(new_ontology_name, 'w', newline='') as new_ontology:\n",
    "        old_reader = csv.reader(old_ontology, delimiter='\\t')\n",
    "        new_writer = csv.writer(new_ontology, delimiter='\\t')\n",
    "        for line in old_reader:\n",
    "            try:\n",
    "                # Match to the desired protein:\n",
    "                protein = prot_ids[line[1]]\n",
    "                phospho_matches = [match for match in phospho_ids if f\"{protein}_\" in match]\n",
    "                for phospho_id in phospho_matches:\n",
    "                    # Add all necessary GO term lines:\n",
    "                    if \"|\" in line[2]:\n",
    "                        GO_list = line[2].split(\"|\")\n",
    "                        for GO in GO_list:\n",
    "                            new_writer.writerow([phospho_id, GO])\n",
    "                    else:\n",
    "                        new_writer.writerow([phospho_id, line[2]])\n",
    "            except KeyError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
